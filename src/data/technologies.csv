id,title,slug,icon,summary,timetoeffect_optimistic,timetoeffect_likely,timetoeffect_pessimistic,example_applications,references,overview,implicationsDefenceSecurity,softwareAndAiRequirements,legalEthicalImplications,strategicImpactAssessment,typeOfTechnology,prominentDevelopers,disruptiveness,sensory,environment,image
1,Touchscreens,touchscreens,1 - Touchscreens Black-01,Small and medium screens (equivalent in size to smart phone screens and video display units associated with personal computers or laptops) which allow the human user to input information via contact or close contact with the screen.,0 Years,2 Years,5 Years,"Projected touchscreens used for mobile computing.

Vehicle control panels for infotainment.

Interactive navigational displays.","- [Touchscreen errors in a military context](https://www.bbc.co.uk/news/technology-49319450)
- [Projected touchscreens](https://www.youtube.com/watch?time_continue=5&v=HT8kJkeECyM&feature=emb_logo)
- [Flexible touch screens](https://www.sciencealert.com/this-new-hyper-flexible-touchscreen-means-your-next-phone-could-bend-and-stretch)","Touchscreens are already widely used in both smartphones and tablets, drawing on capacitive and resistive sensing technologies (but also using technologies such as infrared grids and surface acoustic waves). The human input can be both in the form of simple point and select, finger tracing or incorporating a diverse range of touch gestures based on a standard lexicon (e.g. zoom, swipe, etc.). The presentation of a touchscreen keyboard also allows this technology to be a major source of text information capture.

Developmental advances in touchscreens include the emergence of touchscreen projectors, which project onto a flat surface and turn enable the surface to be interacted with as a touchscreen, the introduction of feedback via the screen to replicate texture and resistance, and the use of close proximity detection, where actual touch is not required. Additionally, the use of infrared sensors in combination with complex algorithms offers the prospect of being able to recognise what is touching the screen and responding accordingly.","- This technology is already being incorporated for simple control panels onboard platforms.

- Different texture effects presented to users could be employed to warn them they are about to select a potentially dangerous option (e.g. discharge a weapon or override a safety parameter).

- Deployable projected touchscreens or foldable touchscreens could be used by dismounted soldiers as interactive situational awareness maps, and potentially allow them to specify where support capabilities should be deployed.","Interpreting user intent with regards to touch gestures and option selection already requires significant AI support for interpretation. As the lexicon of touch expands and becomes more intricate, this interpretation will need to increase in sophistication. It may be that individuals or groups will develop their own language of touch and that AI could play a key role in facilitating this, perhaps through clustering touch gesture patterns or learning to associate them with particular users.","The key areas where legal issues may arise with regards to touchscreens relate to option selection mistakes, task distraction and the use of touch patterns for identity management. Option selection mistakes may need to be considered from a legal perspective, where a critical decision made under duress leads to an erroneous selection of machine system options due to ""fat fingers"". The US Navy has already removed touchscreen interfaces from some ships due to accidents. Design effort to mitigate this risk will be critical, increasing standardisation and reducing complexity. Another area of legal risk may stem from the use of integrated touchscreen interfaces while performing another task simultaneously (e.g. driving a vehicle or operating a machine) resulting in an accident. Finally, the touch patterns of a user could increasingly be used to identify that user and provide continuous authentication for their access to a system. This will raise a number of privacy and data handling issues.","The increasing use of touchscreens will primarily impact on equipment, where in-service platforms will require retrofitting with new control panels, as touchscreens become more standard. Additionally, the logistics for maintaining touchscreens will need to be considered, as on site repairs are likely to be difficult.","Touchscreens rely on resistive and capacitive touch technologies, as well as other technologies such as infrared or surface acoustic technology. They also draw on advanced projection technology.",Tesla are one of the leading companies in developing [touchscreens for vehicle control]( https://www.nngroup.com/articles/tesla-big-touchscreen/). A number of large electronics manufacturers are producing prototype projected touchscreens. [Sony's Xperia Touch](https://developer.sony.com/develop/xperia-touch/) is one prominent example. [Touchscreen International](https://touchinternational.com/applications/military-touch-screen/) is one example of a company specialising in the development of robust touchscreens for a military environment.,Incremental,Visual (text):Visual (non-text),Air:Cyberspace:Electromagnetic:Land:Littoral:Maritime:Space,../images/tech-images/base/1 - Touchscreens.png
2,Touch-interactive large objects,touch_interactive_large_objects,2 - Touch-interactive Large objects Black-01,"Large flat surfaces (such as walls or whiteboards) and irregular shaped objects, which use touch as the input mechanism.",3 Years,5 Years,7Years,"Teaching and learning settings with large displays.

Simple touch operations for devices (e.g. on/off).

Large interactive surfaces for vending devices (e.g. fast food).

Use of building walls for control of equipment inside the room.

Digital interactive signage.","- [Bioimpedance for irregularly shaped objects](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/620c43b2671735343efaeed25fa3ff7a0d62f1ec.pdf)
- [Smart paint on walls for light control](https://www.youtube.com/watch?v=VsGG469gK2g)
- [Smart walls Link #1](https://chrisharrison.net/projects/wallplusplus/wallplusplus.pdf) [Link #2](https://www.youtube.com/watch?time_continue=2&v=etRU_Nst8rE&feature=emb_logo)","This technology category relates to objects other than small handheld or computer screens, which can be touched to provide input to a machine system. It may register simple touch commands, based on different patterns of contact (repeated, moving, duration, etc.) in a similar way to a touch screen, or it may be used to identify a user based purely on characteristics ascertained through touch. 

This technology could also be used to manipulate displays on large surfaces, such as a wall, enabling multi-touch functionality (with multiple users interacting at one time). The current technology primarily utilises capacitive and resistive sensing technologies, but other approaches such as bioelectric impedance have been used, and advances in the development of flexible materials mean that traditional touch technologies are being applied to novel objects.","- This technology could be used on the walls of large naval platforms, to enable environmental conditions to be controlled.

- This could be applied to the handgrips of weapons or other pieces of equipment, to allow identification and authorisation of use.

- Touch technologies may be used in large scale collaborative exercises (e.g. training or military exercises) to allow individuals to manipulate entities in a virtual environment creating shared situational awareness.","Some AI is likely to be required to adapt to human user behaviour, where force, duration, repetition and other touch patterns will need some interpretation to distinguish between intentional and unintentional touches, and also to interpret different commands. Furthermore, if touch sensing is going to be used to identify the individual user, then algorithms that select relevant features and make confidence judgements about the verification of identity will be required.","The primary area of anticipated controversy for this technology is likely to be related to the identification of individuals based on touch. This may raise a number of privacy and personal information issues. Additionally, if system or equipment access is going to be controlled via this mechanism, scrutiny of the confidence of the verification method is likely to be high.","If used in Defence, this technology will need to be integrated into other equipment and therefore could have a significant impact on the need to retrofit existing platforms or equipment.

It may also have an impact on access to information, as it could become integrated into system access controls for IT systems. There is unlikely to be any serious impact on training requirements, as it should be relatively straightforward to use.","This relies on resistive and capacitive touch technologies, but may also incorporate other technologies such as infrared, surface acoustic technology and bioelectrical impedance. Additionally, it will be enabled by the production of flexible electronics.",Companies like [Multitaction](https://www.multitaction.com/) are pioneering the development of large collaborative touch surfaces. [Takram](https://www.takram.com/projects/zensei/) and MIT have collaborated over the Zensei bioimpedance sensor which has been used to identify a user based on contact.,Incremental,Tactile,Air:Cyberspace:Electromagnetic:Land:Littoral:Maritime:Space,../images/tech-images/base/2 - Touch-interactive Large objects.png
3,Gesture and movement sensors,gesture_and_movement_sensors,3 - GESTURE Black-01,Equipment which is capable of sensing and interpreting human movements in free space.,3 Years,5 Years,9 Years,"Game character control/avatar mimicry.

Drone control.

Sign language recognition and gesture-to-speech.

TV control.","- [Very high frequency gesture sensing](https://www.youtube.com/watch?v=Mx-O1nNGz9A)
- [Depth camera gesture sensing](https://www.researchgate.net/publication/254022329_Depth_camera_based_hand_gesture_recognition_and_its_applications_in_Human-Computer-Interaction)
- [Unmanned autonomous vehicle (UAV) piloting Link #1](http://oa.upm.es/26271/1/9084_12_2.pdf) [Link #2](https://core.ac.uk/download/pdf/148684219.pdf)
- [Different uses of hand gesture technology](https://www.researchgate.net/publication/271337997_Survey_on_Gesture_Recognition_for_Hand_Image_Postures)
- [Hand gestures for authentication or identification purposes](https://www.hindawi.com/journals/scn/2018/4879496/)
- [Deep learning for interpreting gestures](https://medium.com/twentybn/building-a-gesture-recognition-system-using-deep-learning-video-d24f13053a1)","This category of technology includes equipment which is designed to capture a human user's physical gestures or other body movements as an input for some sort of machine control or manipulation task. The gestures involved are distinct from those read by other technologies, in that this category primarily concerns gross motor movements performed in free space (either hand arm gestures or whole body actions), and so excludes the reading of facial gestures, or 'near screen' finger gestures, both of which are covered by other technology categories (image recognition cameras and touchscreens respectively). 

The technology currently draws most heavily on optical technologies, such as depth cameras, to sense gestures, but other technologies, such as radar or extreme high frequency waves (which potentially enable gestures to be read through different materials, e.g. a wall) are also being explored. Most future applications being considered are focused on non-safety-critical controls, due to some of the difficulties involved with accurately interpreting user gestures.","- This technology could be used in the remote piloting of platforms or robotics in an intuitive kinaesthetic way.

- Hand gestures might be used as a unique identifier for individuals, being used in parallel or instead of passwords or identity (ID) badges.","Approaches such as deep learning are likely to be required to interpret the gestures accurately, across a community of users with diverse gestural mechanics. Gesture is inherently more ambiguous than, for example, key strokes. This will mean that AI will be required to make more decisions about the intention of the user. The number of gesture features and their potential for variety, is likely to mean that it will be limited as an option for high risk activities within the 20 year timeframe, even given advances in AI technologies over that time.","The key consideration in this area is likely to be the accuracy of the gesture recognition. The more accurate it is, the more likely it is that it will be used for precise and high risk activities (such as targeting and weapon system operation).","This technology is likely to necessitate a training requirement where the technology is used for Defence-specific tasks, which are dissimilar to any everyday uses of gesture recognition.

It is also likely to require substantial amendments to doctrine to standardise the ways gesture sensing technology is deployed in Defence. Work to specify standards will also be needed and there are likely to be a number of policy changes, particularly before the technology has been tested in an operational context.

The technology could have implications for interoperability within alliances and coalitions, as different partners are likely to have different policies on permissions relating to the use of gesture recognition.","Systems draw on a variety of sensors utilising different elements of the electromagnetic spectrum (e.g. radar, optical light and 3D cameras, extreme high frequency waves). Sonar has also been experimented with.","Gesture recognition has been trialled in the gaming community [notably with Xbox's Kinect technology](https://news.xbox.com/en-us/2014/08/27/xbox-one-standalone-kinect/), but has largely been rejected for the time being, while the technology improves. Companies such as [GestureTek](http://gesturetek.com/index.php) and [Sony Depthsensing](https://www.sony-depthsensing.com/) specialise in developing commercial applications of this technology across sectors.",Incremental,Movement/Orientation,Air:Cyberspace:Electromagnetic:Land:Littoral:Maritime:Space,../images/tech-images/base/3 - Gesture n movement controls.png
4,Handheld controllers,handheld_controllers,4 - Handheld Controllers Black,"Handheld controllers (e.g. computer joysticks, game controllers, computer mouse) which are moved and pressed by the human user to provide input.",0 Years,0 Years,2 Years,"Game control.

Drone piloting.

Robot control.","- [Haptic feedback integrated into handheld controller ](https://www.cnet.com/news/sony-reveals-playstation-5-controller/)
- [Motion sensing of handheld controllers for virtual reality](https://partner.steamgames.com/vrlicensing)","Handheld controllers have long been used for computer gaming, ranging from gamepads to joysticks to virtual reality handheld controllers. The games console industry has invested heavily in optimising the design of handheld controllers and there is a very large group of users for whom they represent intuitive input interfaces. 

There have been some trends to integrate handheld controllers with motion sensing technology to augment the game playing experience (e.g. the Wiimote), or to allow interaction with virtual or augmented reality environments, and also to incorporate vibration as a simple form of haptic feedback and introduce adaptive resistance on buttons and joysticks to simulate in game effects. However, in general, technological evolution relating to this type of interface has slowed down recently, suggesting it is fairly mature and less likely to be subject to any further revolutionary innovations.","- Handheld controllers are already used in UAV piloting.

- During the early introduction of robotic capabilities, such as bomb disposal robots, building breach robots or load carrying robots) handheld controls are likely to be used before greater autonomy is afforded to such capabilities.","There is a minimal requirement for AI or software to support the use of handheld controllers at present. However, as the human machine-teaming elements of tasks such as remote vehicle piloting or robot operation become more complex, with partial autonomy, there is likely to be the requirement for solutions which manage the handover between human control and autonomous activity.","There are relatively few legal or ethical issues relating to the use of handheld controllers, which have a history of use and are generally well understood. There may be some issues arising pertaining to culpability for mistakes at the point of transition between human and autonomous control.",Handheld controllers are already in use across Defence and there are unlikely to be any significant DLOD impacts resulting from technological advances in the coming years.,"These devices traditionally provided simple ergonomic push button controllers, but have more recently incorporated a wide range of additional sensors, such as pressure, temperature, touch pads, position/orientation sensors and movement sensors. Furthermore, commercial devices designed for gaming are incorporating a wide range of feedback components such as vibration and haptic actuators.",The game industry is at the forefront of producing handheld controllers. The major games consoles produce handheld controllers for their latest consoles e.g. [PlayStation 5](https://www.theguardian.com/games/2020/apr/07/playstation-5-sony-reveals-new-dualsense-controller-ps5).,Incremental,Movement/Orientation,Land:Air:Maritime,../images/tech-images/base/4 - Handheld Controllers.png
5,Keyboard and text input devices,keyboard_and_text_input_devices,5 - Keyboard Input Black-01,Equipment which is physically manipulated with the primary purpose of entering text into a computer.,0 Years,0 Years,0 Years,"All office-based tasks requiring text input.

Gaming use of a keyboard for character control.","- [Flexible keyboards](https://pubs.acs.org/doi/10.1021/acsami.8b04914)
- [Laser projected keyboard](https://electronics.howstuffworks.com/gadgets/travel/virtual-laser-keyboards.htm)
- [Tilt-based, one handed text entry](https://www.researchgate.net/publication/316709057_Investigating_Tilt-based_Gesture_Keyboard_Entry_for_Single-Handed_Text_Entry_on_Large_Devices)
- [Wearable typing sensors / invisible keyboards](https://www.digitaltrends.com/mobile/tap-strap-wearable-keyboard-news/)
- [Identification and continuous authentication of users based on typing / keystroke dynamics](https://www.researchgate.net/publication/329923359_Recent_Advances_in_User_Authentication_Using_Keystroke_Dynamics_Biometrics)","Physical keyboards are obviously a long-established interface technology. However, there are a number of related newer technologies, designed to capture text input from human users. 

These include a plethora of methods for delivering keyboards to different surfaces, such as garments, paper or any flat surface (using a projected keyboard). Modern manufacturing methods also enable the keyboards themselves to be produced in much more space efficient and portable physical forms, such as folding keyboards and even roll-up keyboards. 

There are some novel approaches being developed for text entry where both hands are not free, such as a tilt-based method for single-handed, fingerless text entry. In addition, less technology-based methods are constantly being explored for one-handed typing, including ergonomically designed one-handed keyboards and chorded keyboards (where multiple buttons are pressed in combination as 'chords' to produce inputs). There are also attempts to develop wearable typing sensors, which require no visual keyboard at all, but instead rely on sensing the movement of a user's fingers. Finally, a number of approaches are being developed to use keystroke dynamics, detected via a standard keyboard, as behavioural biometrics in contexts like continuous user identity authentication.","- The increasing flexibility of keyboards, projected keyboards or technologies that facilitate one handed typing could all be used by an active military role (e.g. a dismounted soldier to produce text while in the field (e.g. a patrol report).

- Analysis of keystroke dynamics could be used to provide continuous authentication of users for classified systems.","The only element of keyboard use that might require AI support is the analysis of keystroke dynamics, where advanced learning algorithms may be used to learn individual patterns in order to recognise users to a sufficient degree of certainty. Additionally, text prediction/auto-complete and spelling/grammar correction algorithms are important supporting software elements which facilitate better and easier text input.",The use of keystroke dynamics might raise some issues around the collection and storage of personal data.,"Capturing keystroke dynamics for users of classified systems, may mean that the data about their individual keystroke patterns could carry the same level of information risk as system passwords. Steps would have to be taken against unclassified systems or personal devices to ensure these patterns were not captured through keylogging software.","Keyboards historically have relied on simple switches which make or break an electric circuit when actuated. However, many commercial solutions exist which remove the need for mechanical components. Projected keyboards for example can detect finger location, movement and vibration. Additionally, it is likely that additional sensors could be built into individual keys to detect hand position and movement as well as temperature and pressure.", [typingdna](https://www.typingdna.com/) are involved in using keystroke dynamics to authenticate users.,Incremental,Visual (text),Air:Cyberspace:Electromagnetic:Land:Littoral:Maritime:Space,../images/tech-images/base/5 - Keyboard Input.png
6,Drawing or writing implements,drawing_or_writing_implements,6 - Drawing or writing implements Black-01,Handheld objects (e.g. a stylus or digipen) whose movement is accurately traced and used as information input.,2 Years,3 Years,5 Years,"The production of artwork.

Architectural design or technical drawing.

Handwriting for taking notes.","- [3D drawing in virtual environments Link #1](https://dl.acm.org/doi/10.1145/3025453.3025474) [Link #2](https://dl.acm.org/doi/10.1145/3025453.3025792)
- [Stylus pens with their own display](https://www.slashgear.com/surface-pen-of-the-future-could-have-its-own-display-17580723/)","This category of technology includes handheld implements that mimic pens or pencils and can be used to draw, write or point / select. This includes the styluses used with some smartphones and tablets, but also extends to longstanding technologies such as light pens.

Recent developments include drawing implements that are used inside virtual environments to draw three dimensional objects. Developments in drawing surfaces are also an important enabler for these interfaces, such as the emergence of digital ink and electronic paper, which allow a more natural sketching interaction to occur. Stylus pens currently in development may incorporate their own displays integrated into the body of the pen, to allow users to select the type of drawing effect they require. These interfaces are increasingly likely to become bidirectional in terms of their information flow, as they begin to relay feedback to their users, relating to features such as texture. For example, some developments are currently focusing on introducing vibro-haptic feedback for erroneous use for educational/training purposes (e.g. where a word is spelt incorrectly with the pen).  ","- These types of implements could be in the design of Defence equipment.

- The use of 3D drawing capabilities could be used in operational planning within a virtual environment.

- Digital pens and paper with in-built translation functions could be used by deployed soldiers in peacekeeping / nation-building operations for communication with the local population.","These implements are quite dependent on being supported by digital drawing software. They may also draw on handwriting recognition technologies, which would learn to recognise the styles of different users. They could be combined with AI translation technologies to convert any text to other languages, as per normal text input.",There are no assessed significant legal and ethical issues relating to the use of drawing or writing implements.,"The DLOD impacts are likely to be minimal, but the broader availability of handwriting and drawing technologies within Defence might require a cultural shift within the organisation, which has become very used to standard business software for the production of text and graphics.","Stylus position on a screen is often similar to touch screens actuated with a finger, using resistive or capacitive sensors on a grid. Light pens are also used, with photosensitive components placed on a layer on the screen. High (pixel level) precision can be achieved with additional optical sensors in the stylus, by indexing individual uniquely coded screen pixels. There are a range of common standard technologies implemented in commercial devices, many of which are under active development.",Companies such as [reMarkable](https://www.takram.com/projects/zensei/) are involved in the production of e-ink drawing tablets which utilise styluses. [Qian Yedan](http://www.qianyedan.com/3d-stylus) is an example of a company producing a 3D stylus for use in augmented reality.,Incremental,Visual (text):Visual (non-text),Air:Cyberspace:Electromagnetic:Land:Littoral:Maritime:Space,../images/tech-images/base/6 - Drawing or writing implements.png
7,Image recognition cameras,image_recognition_cameras,7 - FACE ID Black-01,Cameras which capture information from the human user or their environment as an input (e.g. reading facial expression or observing posture).,2 Years,4 Years,7 Years,"Facial recognition on doorbells or smart devices to control access or customise use.

Emotional detection of human users to adjust system responses.","- [Challenges in facial recognition](https://www.sciencedirect.com/science/article/pii/S1877050918321252)
- [Imagery based detection of human emotion](https://www.sciencedirect.com/science/article/pii/S1877050918321252)
- [Image processing of human operator to control a robot](https://www.sciencedirect.com/science/article/pii/S1877705814033116)
- [Automated thermal image processing for UAV detection of humans](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6068987/)","This includes any camera devices that capture still or moving imagery of a human user and interpret this to provide different system responses. It also includes non-visible imagery capture devices, such as thermal cameras. Where such devices are closely coupled with intelligent image recognition software, they can be used to respond to user inputs, such as facial expressions, posture, gait and other body movements in order to provide tailored or adjusted system outputs. 

Most of the technological advances are currently being achieved in the underlying image recognition technologies, such as deep learning, rather than in the camera devices themselves. However, advances in the hardware, such as miniaturisation, may enable new applications for image recognition interfaces. Additionally, the advance of different imaging techniques (e.g. thermal scanning) may enable these interfaces to absorb much more information from their human users than is present in the visible spectrum, potentially allowing insights into mood and health.","- Facial recognition could be used as a verification method for secure IT system access, but also site access.

- Monitoring of stress and fatigue via camera-captured imagery could be used to help determine human operator status, and therefore increase or reduce autonomous support.","Advanced algorithms have already been developed for commercial applications of facial recognition software, such that confidence is very high with sufficiently high quality imagery. This will continue to improve, also enabling facial expressions to be accurately read by machines (possibly even more accurately than by humans).","The key legal and ethical issue pertaining to this interface technology is to do with data privacy, when it relates to non-consensual applications which can be used to determine the movements and activities of a person. Similarly, there are some issues relating to machines sensing and responding to human emotional states, as assessed through analysis of facial expression of other visible tells.",Establishing the infrastructure and equipment to enable this technology to be deployed across Defence would be a significant undertaking. There would also be a requirement for a substantial degree of staff consultation to ensure personnel were content with its intended use.,"Commonly available cameras use Complementary Metal-Oxide-Semiconductor (CMOS) integrated circuits, either with a pinhole or a set of optical lenses (diffraction and/or fresnel). These optically sensitive components can be designed to operate in a range of optical frequencies considerably wider than the visible light spectrum. For example, cameras on modern laptops can image in the near infrared spectrum to indicate the temperature of the subject. Far Infrared camera components are available (albeit expensive and bulky), and are used in security applications.","There are numerous companies specialising in image recognition technologies, principally focusing on the underlying AI, rather than the cameras themselves. [AnyVision](https://www.anyvision.co/) and [Herta Security](https://www.hertasecurity.com/en) are two prominent examples.",Step,Visual (non-text),Land:Air:Maritime,../images/tech-images/base/7 - Image Recognition Camera.png
8,Eye tracking equipment,eye_tracking_equipment,8 - EYE Tracking Black-01,Technology which captures human eye movement as an input.,4 Years,6 Years,8 Years,"Measuring online advertising effectiveness by observing viewing patterns.

Improving virtual reality experiences.","- [Insights to be gained by eye tracking](https://link.springer.com/chapter/10.1007/978-3-030-42504-3_15)
- [The anatomy of eye movements](https://www.researchgate.net/publication/265162808_Human_Eye_Tracking_and_Related_Issues_A_Review)
- [The application of eye tracking in aviation](https://www.tandfonline.com/doi/full/10.1080/24721840.2018.1514978)","This includes the monitoring of user eye movements as an input, allowing systems to respond to visual focus. Eye tracking can be delivered through a number of technologies, although most modern systems principally use reflected infrared light. Eye tracking is used in two main ways. First, to infer user interest and focus (in terms of what they are looking at) and secondly to allow deliberate hands-free interaction, where conscious staring is used to select content or options (eye tracking for control). Systems can be head mounted or placed in front of a user, but soon eye tracking may become a standard feature on smartphones and many other devices. 

Some of the key developments in this area will come not from the eye tracking itself, but from using this information to infer the intent of a human user of a system and provide predictive responses accordingly. This could see the provision of seemingly intuitive support from machine systems, where they are able to filter information on the user's behalf and anticipate the options of relevance to the user.","- Eye tracking could potentially be used to assist with visual targeting operations, where weapons systems follow the user's line of site.

- It could be used to help surface relevant information to users quickly based on a dynamic display of operational activity.","The data analysis required to actually track eye movements is relatively mature. The next steps towards extracting additional utility out of eye tracking interfaces will involve trying to interpret the intent behind that eye movement (drawing in information like gaze time, focus and pattern of eye movement) to try and model the purpose of a particular 'look'.","There are some ethical issues around monitoring eye gaze and the insight this provides into a human's interest or intent. It may be possible, for example, to determine human attraction based on gaze. Legal and policy challenges may focus on the provision of consent and the protection of private thoughts and feelings.",Establishing the infrastructure and equipment to enable this technology to be deployed across Defence would be a significant undertaking. There would also be a requirement for a substantial degree of staff consultation to ensure personnel were content with its intended use.,"These systems generally rely on an image of the user's eye, usually in Infrared (to prevent dazzling). The image, along with a known reference point (e.g. the position of the sensor) is used to extrapolate the gaze direction of the user's eye. Additionally, reflections of light, or distortions of the image of features of the eye (e.g. iris or retina textures) can be used to estimate the momentary focal distance and direction of the eye.",[Tobii](https://www.tobii.com/) is one of the leading companies specialising in eye tracking interfaces.,Step,Movement/Orientation,Land:Air:Maritime,../images/tech-images/base/8 - Eye Tracking equipment.png
9,Physiological monitors,physiological_monitors,9 - Physiological monitors Black-01,Devices or equipment which capture physiological measures or biometric indicators from the human user as a system input.,5 Years,7 Years,10 Years,"ID verification via biometrics.

Prescribing tailored exercise to users based on their physiological status.","- [Overview of health monitoring devices](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6111409/)
- [Wearable physiological monitors for healthcare](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6111409/0)
- [Integrated physiological monitoring in a military context](https://cradpdf.drdc-rddc.gc.ca/PDFS/unc256/p804911_A1b.pdf)
- [Photoplethysmography for detecting divided attention](https://dl.acm.org/doi/10.1145/3025453.3025552)
- [Breathing and posture monitor](http://prana.co/)","This category of interfaces is based on a large number of existing monitoring technologies which have, in some cases, long-standing applications in healthcare settings. This could include using heart rate, blood pressure or other indicators of physiological stress as inputs to a system, which adapts its response. Alternatively, it might involve using biometric markers (e.g. thumb print, iris, DNA) to identify a user and permit or tailor their system access. 

Emerging approaches to the capture of user inputs based on physiological markers include, monitoring skin blood perfusion through photoplethysmography and wearable breathing and posture monitors. The technology related to the analysis of this data is also rapidly advancing, with the potential for better risk identification and anomaly detection coupled with continuous data collection, promising a personalised healthcare revolution.","- Group physiological monitoring could be used to control environmental conditions on a large platform.

- Monitoring the physical and emotional health of deployed personnel to support the automated deployment of healthcare and pastoral interventions.",These interfaces could be combined with autonomous assistants to allow them to provide tailored pastoral support.,"The routine use of physiological monitoring may mean that many of these interfaces will be subject to the MOD Research Ethics Committee (MODREC) approval throughout their development process. Once in service, there is likely to be an ongoing issue related to the provision of informed consent for continuous physiological monitoring of the user, particularly where there is an invasiveness to the sensors itself.",These types of interface are likely to require the establishment of policy and IT infrastructure to ensure the privacy of information derived from collected physiological data.,"Physiological monitors can employ a very wide range of sensor types. Simple sensors include temperature and piezoelectric pressure components. Commercial and consumer grade biochemical spectroscopy and DNA sequencing solutions are emerging and decreasing in cost and size, and it is likely that such components will become commonplace in consumer devices (in a similar way that Global Positioning System (GPS) components are now ubiquitous).","[Fitbit](https://www.tobii.com/) are one of the most high profile producers of interfaces which take physiological measures (principally heart rate, but also looking at other measures).",Disruptive,Electrical Signal,Land:Air:Maritime,../images/tech-images/base/9 - Physiological monitors.png
10,Physical virtual reality control interfaces,physical_virtual_reality_control_interfaces,10 - Physical Virtual Reality Control Interface Black-01,Real physical objects used by an individual interacting with virtual reality to provide a physical real-world effect (e.g. stationary treadmills or token objects).,4 Years,7 Years,12 Years,"Creating proxies for virtual game objects in the real-world.

A stationary treadmill and harness used for moving around in a virtual world.","- [Physical objects in a mixed reality context](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6068987/)
- [Flexible touch tokens](https://dl.acm.org/doi/10.1145/3025453.3025894)
- [Virtual props for virtual climbing](https://www.youtube.com/watch?v=5iVaj1CPDXo)
- [Robotic assembly of proxy objects for virtual reality interactions](https://www.youtube.com/watch?v=lac2-orl9CI)","A human user operating in a virtual world has a limited ability to interact intuitively in a physical sense with their virtual surroundings. This category of interfaces brings together all of the real-world objects and pieces of equipment which are utilised to enable a user to physically interact within a virtual environment. This might include equipment which allows a user to experience physical movement or action within the virtual world (such as a stationary treadmill) or it might include physical representations (tokens or proxies) of objects in the virtual world which are interacted with (e.g. a physical mock-up of a control panel which exists in the virtual space). 

As innovations in mixed reality continue to flourish, the fusion between physical props and equipment, and the virtual environment are likely to increase. Approaches like the robotic assembly of proxy objects for use in virtual environments, could enable increased flexibility in the provision of real-world objects, which match virtual objects. Additionally, tracking technology which helps situate different individuals within the same virtual space may begin to enable virtual environments to be spatially synchronised with real-world props for multiple users. ","- These kinds of interfaces could be for detailed preparation for special forces operations, where a particular physical space is mocked up, but the dry-run is also conducted in a virtual simulation for added realism.

- A stationary treadmill could be used for physical exercise in a virtual environment, where there are no opportunities for outdoor exercise (e.g. onboard a submarine).",Matching the real physical objects and placing them within the virtual environment is obviously entirely dependent on the software that generates the virtual environment.,"It is possible that users may be injured by a physical object that is used to enhance a virtual environment. This risk will have to be assessed and managed, and could lead to personal injury claims. Activities considered to be low risk in the real-world (e.g. walking), might carry much greater risk when performed in a virtual environment.","These physical devices present some interoperability issues, because they can only be utilised in one location at a time. This means different users entering a virtual environment from different physical sites might not be able to coordinate their experiences.","The production of most physical objects that are used to provide a physical presence to support virtual environment immersion, is fairly low tech at present (largely consisting of traditional construction methods). However, rapid 3D printing and autonomous robotic construction using building blocks (presently at low resolution), may be employed to support near-instantaneous generation of physical objects.","The [Virtuix Omni](https://www.virtuix.com/) is one of the best known virtual reality treadmills, with other companies producing similar equipment for use in the games industry. [Birdly](https://birdlyvr.com/) have also developed a novel physical flying interface.",Step,Movement/Orientation,Land:Air:Maritime,../images/tech-images/base/10 - Physical Virtual Reality Control Interface.png
11,Smart clothing and other wearables,smart_clothing_and_other_wearables,11 - Smart clothing Black-01,"Clothing or worn adornments (jewellery, watches, etc.) which capture information from the user based on their activity, and which may present information to their wearer.",4 Years,6 Years,11 Years,"Wrist worn activity monitors for health and exercise.

Adaptable display garments or jewellery.","- [Smart posture-sensing clothing for exercise](https://vimeo.com/275497791)
- [Smart fingernails](https://www.researchgate.net/publication/316712312_AlterNail_Ambient_Batteryless_Stateful_Dynamic_Displays_at_your_Fingertips)
- [Interactive tattoos](https://duoskin.media.mit.edu/)","There are a number of rapidly emerging innovations in this interface technology, which includes clothing and other worn adornments used to capture human input, and worn devices that present information back to the user and other people. 

The development of durable and flexible electronics is facilitating a boom in smart clothing, which is able to capture body movement and feedback with either visual display changes or haptic effects. This also includes the category of smartwatches and activity monitors, which could further extend their adoption among the public if they are integrated with functions such as contactless payments and identity verification. Another niche area of wearable interface development is in smart body decoration through features such as smart tattoos and fingernails.","- Smart clothing could be used to monitor the location, activity and status of personnel in the field.

- Smart ID cards could be used by individuals accessing and moving around MOD sites. These might control access and display information about permitted activities.","There is a significant requirement for data analytics to interpret information collected by the wearable device (determining things like movement and posture from interactions with the device). Where the wearables are used for display purposes or providing other information to the human user, there is then a need for some interpretation of what the correct output should be via the wearable interface (e.g. what visual display should result following a particular captured output?).","The proximity and persistent presence of wearable technologies, means that there are likely to be a number of issues relating to the right to privacy for personnel who are using these interfaces.","Personnel are likely to need consulting before these kinds of interfaces are used because of potential concerns about the right to privacy. The collected information will also need carefully protecting, because it will have both data privacy and security risks attached to it. Because these interfaces will be worn, they are likely to be issued as personal equipment. The distribution and administration of this will need to be planned.","Aside from sensors common to other interfaces for measuring position and movement, these interfaces may also include smart materials which can directly measure deformation, temperature, pressure and other features through electrical or chemical changes within the material itself. Similarly, smart materials can be designed to behave in predictable and useful ways when electrical current or chemical changes are introduced. For example, such materials may become stiff or change colour under stimulation.","[Wearable X](https://www.wearablex.com/) is a high profile clothing and technology company has developed a number of smart clothing ranges, including Nadi X to assist with yoga posture.",Step,Tactile,Land:Air:Maritime,../images/tech-images/base/11 - Smart clothing.png
12,Telepresence equipment,telepresence_equipment,12 - Telepresence Black-01,Equipment used to enhance the sense of being in another location to support activities such as videoconferencing.,1 Year,3 Years,8 Years,"Business teleconferencing.

Support of multiplayer gaming communications.","- [Telepresence to support teleconferencing](https://www.frontiersin.org/articles/10.3389/fict.2015.00008/full)
- [Telepresence robots in teleconferencing](https://ri.cmu.edu/pub_files/2011/7/11robocup-telepresence.pdf)","Most telepresences are currently conducted through standard IT hardware, and this is likely to provide the primary interface for some time to come. The global 2020 Covid-19 pandemic has seen a rapid upsurge in remote working and telepresence adoption. A number of telepresence software applications have seen rapid growth in this time. 

However, there are a number of human-machine interface technologies being developed that aim to try and account for some of the deficiencies in the human experience that seem to persist with teleconferencing. The use of telepresence robots, which provide a mobile interface and a proxy physical presence for the remote participant, has yet to take off widely, but a number of reasonably mature products exist. The integration of other technologies such as 3D sound may also occur in the future, in order to help participants distinguish between speakers.","- This equipment could be used to provide a physical presence for a commander issuing orders from a headquarters into theatre.

- Telepresence interfaces might be used to support deployed personnel with expert advice for high skilled tasks (e.g. administering medical treatment or repairing specialist equipment).","Generally, the software requirements for these interfaces are fairly minimal, with the technology largely conveying the audio and video feed of human users via different equipment. Where telepresence avatars are used, there may be some need for emotional interpretation so that the avatar can reflect the emotions of the non-present participant.","These interfaces may make the provision of expert advice (via technology) to other humans acting in proxy more common. This could start to blur the boundaries of responsibility for action and decision-making in new ways, where it becomes less clear whose fault a particular error might be.","One of the most significant DLOD areas affected by developments with these interfaces could be training. First, they will enable training to be delivered more flexibly. As the technology extends the opportunities for physical interaction between remote users, this may mean more types of training can be done at distance. Second, it may reduce the need for the number of people that will need training for some highly skilled roles, by removing the need for their physical presence in order to provide their skill or function. For example, they may simply be able to instruct other people on site to carry out certain tasks, reducing the amount of travel time and allowing them to serve multiple sites.","Traditionally limited to audio and visual technologies, these interface systems are being extended to provide 'tele-haptics', such as remote surgical systems, which provide realistic touch feedback to the user.","The [Padbot P1](https://www.padbot.com/) is an example of a budget telepresence robot, which provides a mobile screen presence for teleconferencing. [Appbot Riley](https://www.rileyrobot.com/over/) is a small robotic camera, which provides you with a video feed of remote locations.",Step,Audiovisual,Land:Air:Maritime,../images/tech-images/base/12 - Telepresence.png
13,Speech-based interfaces,speech_based_interfaces,13 - Speech based interface Black-01,Devices designed to facilitate speech-based human machine interaction (e.g. smart speakers or other voice-activated devices).,2 Years,5 Years,8 Years,"Smart speakers for music and limited domestic management.

Accessing services while driving.","- [Ethical issues relating to smart speakers voice assistants](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/831180/Snapshot_Paper_-_Smart_Speakers_and_Voice_Assistants.pdf)
- [Voice assistants market trends](https://ec.europa.eu/growth/tools-databases/dem/monitor/sites/default/files/Virtual%20personal%20assistants_v1.pdf)
- [Military applications of voice assistant technology](https://www.voicesummit.ai/blog/how-the-military-is-using-voice-technology)
- [Potential voice assistant applications for naval platforms](https://www.theguardian.com/uk-news/2017/sep/12/british-navy-warships-to-use-voice-controlled-system-like-siri)
- [Usability of smart speakers for military personnel with mild traumatic brain injuries and post traumatic stress disorder](http://scholarworks.csun.edu/bitstream/handle/10211.3/202991/JTPD-2018-ID16-p127-139.pdf?sequence=1)
- [Voice assistants trained with deep learning to reduce surgical error](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231322)","The use of smart speakers and voice-activated assistants via smartphones or computers have dramatically increased. While the interfaces essentially continue just to be an integrated microphone and speaker (albeit underpinned by advanced AI to recognise speech and deliver appropriate voice output), it is likely that there will be a number of interface developments over the next decade. 

These could include the integration of visual displays and touch technologies with speech-based interfaces, and the miniaturisation of equipment for inclusion in wearables. Additionally, further advances in the ability to reliably interpret and process human speech and provide intelligent responses will improve their effectiveness. This might include improvements in their ability to recognise regional accents and dialects, and also interpret the tone of voice related to non-verbal clues about meaning (e.g. the mood of the speaker). Natural language processing and translation capabilities could lead to the development of highly effective interpretation interfaces, transforming cross-cultural communications. Speech-based interfaces are likely to become ubiquitous due to their ease of use.","- This technology could be used to provide personnel with smart assistants to aid complex work, where constant reference to sources is required (e.g. deployed maintenance of equipment).

- This type of technology could be used to help produce live translation for deployed personnel operating among a population of non-English speakers.","There are significant requirements for AI support in the areas of speech recognition, natural language understanding and natural language generation underpinning the functionality of these interfaces. Additionally, proffering the right responses to a user based on what they have requested is an extremely complex task, requiring a very general sort of AI. With regards to these interfaces and the services they offer, more than almost any other interfaces, the user is going to expect transference across devices (i.e. they will expect to 'speak' to the same smart assistant, whatever actual device they address). This means that the user will probably become less aware of the interface mechanism and more focused on the software sustaining and developing the smart assistant.","One of the key ethical issues regarding these devices will be the potential for humans to develop meaningful relationships with the smart assistants sustained by these interfaces. Even though they are currently disembodied, their level of sophistication and complexity is reaching the point where they start to feel like a human assistant. There are then potential ethical issues about the treatment of the assistant, or at least the users' perceptions of the treatment of the assistant. There are also other issues related to these devices being used to record audio from the user's environment and the implications for privacy, as well as issues to do with trust and responsibility for the guidance or instructions provided by a digital assistant.","Doctrine would need to adjust significantly to provide guidance on how and under what circumstances smart assistants might be used. The impact on personnel would also need to be studied as the technology advances, to assess the impact on their emotional state and wellbeing - with events such as updates potentially becoming a source of stress.","Generally these interfaces use audio speakers and sensors. However, they can be augmented by sophisticated audio processing to infer and apply directionality to the sound. Additionally, in combination with other sensors (such as user position) they can provide a richer experience.","Most of the technology giants produce their own smart speakers, notably [Amazon's Echo](https://www.amazon.co.uk/amazon-echo-3rd-generation-smart-speaker-with-alexa/dp/B07P4DKX14) and the [Sonos One](https://www.sonos.com/en-gb/shop/one.html) (which incorporates Google's Alexa service) and also offer their own supporting smart assistants. 
Vehicle-focused services are also being developed, such as [Apple CarPlay](https://www.apple.com/uk/ios/carplay/).",Step,Audio (speech),Land:Air:Maritime,../images/tech-images/base/13 - Speech based interface .png
14,Vehicle controls,vehicle_controls,14 - Vehicle controls Black-01,"Physical equipment, internal to a vehicle, which is manipulated by the user to control the vehicle, and which employs some intermediate processing (i.e. is not purely mechanical).",2 Years,5 Years,11 Years,"Driving a personally owned car.

Driverless public transport with emergency manual overrides.","- [Societal implications of autonomous vehicles](https://www.jtlu.org/index.php/jtlu/article/view/1405/1209)
- [Decision-making challenges in autonomous driving](https://www.tandfonline.com/doi/full/10.1080/08839514.2019.1600301)
- [Voice controls in vehicles](https://www.automotiveworld.com/articles/digital-voice-assistants-are-the-future-of-in-vehicle-control/)
- [Resuming manual control from an automated vehicle](https://www.sciencedirect.com/science/article/pii/S1369847814001284)","Advances in the interfaces for vehicle control are primarily being driven by the automobile sector, as commercially available cars increasingly take on smart and autonomous features. However, aviation, with a longer history of smart interface development, will continue to contribute innovations. 

The key catalyst determining how interfaces will change, will be vehicle automation, where interfaces are likely to move away from direct control towards enabling human-machine partnering for partial autonomy. 

Eventually these interfaces will focus simply on capturing user intent and providing optimal environmental conditions and infotainment options which have not previously been available because of the requirement not to distract the human driver. Interface technologies that are likely to be increasingly used in this area are voice-control for navigation and infotainment, haptics for feedback to assist with driving or flying conditions, and biosensing technologies for verifying identity for access to vehicles (the latter becoming particularly important in a future scenario where personal vehicle ownership decreases).","- These technologies could be used for the piloting or operation of any manned platform.

- There are particularly significant implications for areas like logistics, where vehicle fleets might not require human drivers.","The six levels of driver automation developed by SAE map out a progression towards fully autonomous vehicles [Ref.9]. At each stage the potential automated features require intelligent processing of sensor information and algorithmic decision-making. This obviously becomes more extensive, integrated and complex as greater autonomy is achieved, requiring ever increasingly sophisticated AI.","Most of the legal challenges around vehicle control interfaces and autonomous vehicle control have focused on where fault lies in the event of an accident (the driver or the manufacturer). There are also ethical issues that arise relating to decision-making and minimising human harm, specifically deciding whose protection should be prioritised by automated responses.","This technology has very significant implications for areas like training and personnel, where activities like pilot training may be dramatically affected or where the role of 'driver' may be removed altogether.","New interfaces for vehicles have historically attempted to augment or improve established controls and instruments for the type of vehicle. For example, enhancing fly-by-wire systems in aircraft and the wheels and pedals in road vehicles with additional sensors and user feedback, and replacing mechanical instruments with digital displays. However, with the introduction of increasingly sophisticated autonomous systems, the interfaces used in vehicles are having to shift towards communication technologies (e.g. speech recognition).","[Tesla] (https://www.tesla.com/en_GB/autopilot) have been at the forefront of vehicle control innovation over the past decade. However, most other automobile manufacturers have now started introducing new interfaces to integrate with growing amounts of autonomy. 

There are also a number of shifts in the standard control vehicle control paradigms, including the introduction of the [Nissan Leaf e-Pedal](https://www.nissan-global.com/EN/TECHNOLOGY/OVERVIEW/e_Pedal.html), which combines acceleration and braking through a single mechanism.",Step,Movement/Orientation,Land:Air:Maritime,../images/tech-images/base/14 - Vehicle controls.png
15,Robot Controller Interfaces,robot_controller_interfaces,15 - Robot controller interfaces Black-01,Interfaces specifically designed to allow the human to control of robots or robotic components.,1 Year,4 years,6 years,"Robot use in manufacturing. 

Robots used to perform actions in hazardous environments. 

Robot operation in distribution warehouses.","- [Robot control using haptic telexistence](https://tachilab.org/en/projects/telesar-v.html)
- [Image processing for robot control](https://www.sciencedirect.com/science/article/pii/S1877705814033116) 
- [Master-slave robot operations for surgery](https://www.tandfonline.com/doi/full/10.1080/24699322.2016.1240313) 
- [Trends in robot controller interfaces](https://www.researchgate.net/publication/261334489_New_trends_in_industrial_robot_controller_user_interfaces) 
- [Demonstrations of human-robot handover](https://www.youtube.com/watch?v=Ac4kgipC7A0)","Robots are controlled by a variety of interfaces which are covered by other technology areas within the catalogue, such as handheld controllers, keyboards, speech-based interfaces and other technologies, like image processing of the human operator's movements. This technology area, though, is focused primarily on those interfaces that are embedded within the robot itself or have been developed purely to support tele-operation. 

These interfaces have traditionally included joysticks, levers or buttons, but are increasingly adapting to new paradigms, such as master-slave relationships and human-robot handovers for physical co-location and teaming. Master-slave relationships are fuelling the development of increasingly sophisticated control and feedback rigs, where full human limb motion is enabled and tracked by the rig and translated into robotic actions which accurately mimic the human action. The integration of haptic feedback based on the robot's interaction with objects promises to enable touch feedback, facilitating micro motor tasks and delicate operations. The improving sensitivity of human-robot handover opens up possibilities of humans working safely alongside robots, passing objects between one another for manufacture or robots manually handling heavy items, while they are worked on by human craftspeople.","- Robots have already been extensively used in support of bomb disposal tasks. 

- Robots are increasingly being examined in terms of military load bearing activities in support of dismounted soldiers. 

- Robots partially controlled via interfaces could increasingly become a part of routine, low skilled activities like loading for logistics and cleaning onboard naval platforms.","It is probable that any robots being controlled via interfaces will have some autonomous elements to their performance as well. For low risk, repetitive activities such as cleaning, it is likely that most of this will be conducted autonomously, with any interfaces primarily being for manual override or programming the task in the first place. There may be less autonomy for higher risk and more complex tasks, such as bomb disposal or surgery. However, even for these tasks, software is likely to play some role in the performance of the task by the robot.","Robots, particularly if anthropomorphic, may become valued members of a team. There could, therefore, be some moral issues that arise related to duty of care for the robot, particularly if it becomes linked to group cohesion and morale. Issues of responsibility, fault and safety will also need to be considered when robots are deployed, particularly where there is a mixture of human control via an interface and autonomous action.","Operating robots via interfaces will require careful consideration of data feeds (information), logistical support of maintenance of spares for the robot, and training of robot operators (both in terms of control via the interface, but also the rules of engagement and doctrine for their use).","Robotic systems rely on common place sensors and input devices such as pressure and positional sensors to infer the operator's intent, as well as a range of sophisticated motors and other mechanical components. Increasingly, systems provide feedback to the user in the form of haptics. Capturing timely information from the user and providing feedback to them, rely on rapid and sophisticated signal processing.","Japan provides many of the leading robotics companies and research institutions, many represented by the [Robotics Society of Japan](https://www.rsj.or.jp/en/). [Boston dynamics](https://www.bostondynamics.com/) have been particularly prominent in the development of robots in a military context.",Incremental,Movement/Orientation,Land:Air:Maritime,../images/tech-images/base/15 - Robot controller interfaces.png
16,Exoskeletons,exoskeletons,16 - Exoskeletons Black-01,Robotic components connected externally to the human body to reinforce or augment human physical performance.,5 Years,9 Years,14 Years,"Exoskeleton use for rehabilitation in stroke patients. 

Exoskeleton use in construction.","- [Exoskeleton control systems](https://www.sciencedirect.com/science/article/pii/S1877705812026732)
- [Human-machine interfaces for exoskeletons](https://lib.dr.iastate.edu/cgi/viewcontent.cgi?article=1175&context=imse_pubs) 
- [Review of exoskeleton applications](https://lib.dr.iastate.edu/cgi/viewcontent.cgi?article=1175&context=imse_pubs) 
- [Example human augmentation exoskeleton](https://www.sarcos.com/products/guardian-xo-powered-exoskeleton)","Exoskeletons are different from the standard robotic interface paradigm. With an exoskeleton, the robotic elements are co-located with the human user (i.e. they are in contact with the human user), in contrast with the remote operation of a robot. Exoskeletons have two primary applications which help to determine the most appropriate forms of control interface. These main applications are rehabilitation and performance augmentation. In the case of rehabilitation, the user may have limited physical control due to injury or illness, and so often the appropriate interface is one which directly receives inputs from the human nervous system (e.g. via EMG). Where human performance augmentation is concerned, there is usually more scope for more deliberate control interfaces, such as built in hand controllers. 

As exoskeletons become less bulky and more adaptable in their function, interface technology will also have to advance. For example, a multifunctioning exoskeleton is likely to need an advanced brain-computer interface to make it a practical technology. Additionally, as material science advances, the exoskeletons themselves are likely to become lighter weight and more flexible. The potential for flexible exoskeletons, which overlap with smart clothing, could offer 'worn' exoskeletons to assist with applying additional mechanical torque when required. As human exploration of space and underwater environments increases, larger more complex exoskeletons, which would be impractical in a land-based environment, could come into operation, driving interface design towards hands free and low visual interfaces.","- Exoskeletons used to support with military engineering tasks.

- Exoskeletons used to rehabilitate injured personnel and veterans.

- Exoskeletons applied to augment close combat tasks","Exoskeletons will require sophisticated interpretative analytics in order to read the command signals of the human user. Without this, it is unlikely that exoskeletons will be able to achieve anything other than simple, repetitive actions. To unlock their true potential, they will need to be able to receive and process complex inputs (perhaps based on BCIs or other neural inputs, or a wide range of physical movement inputs). Meeting this challenge is likely to involve deep learning technologies, which will be able to account for individual differences in input.","The primary legal and ethical concerns relate to human augmentation. First, in integrating robotic components with a human user in this way, there is a danger of blurring the distinction between the equipment and the person. This close coupling could lead to a greater focus on the maintenance and capability of the exoskeleton at the expense of human wellbeing (this could also have implications for rules of engagement when targeting exoskeletons). Second, there are attendant risks in making humans more capable, whether in ameliorating the effects of injury or disability, or augmenting the uninjured/able bodied. Should exoskeletons become more available, issues relating to who has the right to access this technology may arise (particularly as it is likely to be expensive).","The operation of exoskeletons will require a high degree of training for the human user. The equipment itself is likely to be expensive, so a significant proportion of this training may need to be virtual or simulated. Keeping exoskeletons maintained in hostile environments will require a significant logistical effort. Doctrine may need adjusting to take into account both the operation of exoskeletons and the engagement of enemy exoskeleton capabilities. ","Exoskeleton systems often rely on common place sensors and input devices such as pressure and positional sensors to infer the operator's intent. Increasingly, systems (particularly where used to augment human capability) provide feedback to the user in the form of haptics. Capturing timely information from the user and providing feedback to them, rely on rapid and sophisticated signal processing.","[Sarcos](https://www.sarcos.com/products/guardian-xo-powered-exoskeleton) are one of the main producers of commercially available exoskeletons, with a range of interface controls.",Step,Movement/Orientation,Land,../images/tech-images/base/16 - Exoskeletons.png
17,Body-integrated interfaces,body_integrated_interfaces,17 - Body integrated interfaces Black-01,"Interfaces embedded within the human body or which take inputs directly from the human nervous system, including brain-computer interfaces and bionic prosthetics.",5 Years,12 Years,18 Years,"Bionic prosthetic devices.

Embedded sensory devices for visually impaired or hard of hearing.

Enabling those with paralysis to operate equipment.","- [Bionic hand with haptic feedback](https://www.sciencedaily.com/releases/2019/07/190724144150.htm)
- [EEG brain-computer interfaces](https://journals.sagepub.com/doi/full/10.26599/BSA.2018.9050010)
- [Composing music with brain-computer interfaces](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0181584)
- [Military considerations for brain-computer interfaces](https://thejns.org/focus/view/journals/neurosurg-focus/28/5/2010.2.focus1027.xml)
- [Monitoring brain activity during firearms exercises](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7179757/)
- [Human augmentation legal and policy issues](https://www.brookings.edu/research/our-cyborg-future-law-and-policy-implications/)","This category of interface technology includes any devices that are embedded within the human body and/or take their input directly from the human nervous system. This includes those interfaces which capture neural signals non-invasively (such as electroencephalogram (EEG) and electromyography (EMG)). At present, because of ethical and safety concerns, the use of such interfaces has largely been limited to users with physical disabilities such as paralysis, limb loss or people rehabilitating from a stroke. 

However, as the risk to the individual reduces (presumably as the technology advances and confidence in the installation procedures increases), then these technologies will start to open up a number of very disruptive options, such as the direct reading of human intent. Research into P300 brain-computer interfaces, which offer the possibility of reading human attentional focus or even selection of options directly from brain activity, has increased and has the potential to allow people to write, paint and even compose music through thought alone. Additionally, embedding psychophysiological sensors into the human body, opens up the possibility of constant monitoring, reacting to and even controlling emotional and physical responses to stressful situations. Even more radically, these technologies could ultimately be used to input to the human nervous system, generating movement, physiological responses and even stimulating thought through external input.","- Bionic prosthetic limbs have already been used for injured military personnel and veterans.

- Brain-computer interfaces could be used to dynamically interpret commander intent for complex and rapidly evolving tasks involving partially autonomous elements (command of drone swarms).","Analysing the electrical activity of motor neurons and translating this as voluntary control of bionic prosthetics is a relatively straightforward analytical task, with only a small number of variables to be considered, such as amplitude, frequency and duration. However, interpreting human brain activity and inferring intent is likely to be a highly complex analytical task, which will require advanced AI to carry out.","This interface technology is fraught with legal and ethical challenges. These stem from both the risks associated with invasive surgery and also issues surrounding the interpretation of commands or intent based on neurological signals. There are philosophical questions to be dealt with over whether there are differences between reading motor commands from a neuron and pressing a button, but also more fundamental questions about whether intent is the same as action, and the broader issue of defining human agency. Additionally, there are unprecedented privacy challenges to be considered relating to the potential to gather information directly from the human brain. Even though the technology is unlikely to be able to 'read minds' in the near future, establishing differences between neural and mental data will be crucial as the technology develops.","These interfaces will require specific medical interventions and legal frameworks (concepts) to enable them to become widespread. The logistical support and information supply to embedded interfaces will need to be uninterrupted, because the human user is likely to be significantly affected by any downtime. A new paradigm for support will be required as humans in effect become an integral part of infrastructure.","External sensors such as EEG and ECG sensors detect electrical impulses in the nervous system. These sensors can detect the position (in 3D space) of the impulse as well as their magnitude. Generally, the resolution of the interface is sensitive to the number of sensors and the degree of intimacy of contact with the subject. Direct sensing is similar, except electrical impulses are measured with direct electrical contact with the nervous system. Interfaces may use traditional non-organic chemistry components (such as metals or semiconductors). However, these may be quite invasive to the subject, and may not provide long-term viable interfaces - e.g. the immune system of the operator may react to their presence. Bioelectronics and biochemical based sensors may provide a less invasive solution, interfacing in a more 'natural' way with the subject.","Research groups like the [Next Generation Neural Interfaces (NGNI) Lab](https://www.imperial.ac.uk/next-generation-neural-interfaces) at Imperial College London are at the forefront of research in this area.

[OpenBCI](https://openbci.com/) have produced a set of open source tools used by the academic and industry communities.",Disruptive,Electrical Signal,Land:Air:Maritime,../images/tech-images/base/17 - Body integrated interfaces.png
18,Smart home devices,smart_home_devices,18 - Smart home devices Black-01,"Household equipment which has a defined and limited purpose (e.g. clocks, kitchen appliances, heating thermostats) and which incorporates computer processing as part of its functioning.",2 Years,5 Years,8 Years,"Integrated kitchen appliances.

Home security and protection systems.

Home entertainment systems.","- [AI in support of smart homes](https://www.mdpi.com/2624-6511/2/3/25/htm)
- [Survey of smart home technology uses and perceptions](https://www.sciencedirect.com/science/article/pii/S030142151630711X)
- [Longer term uses of smart home devices](https://www.tandfonline.com/doi/full/10.1080/09613218.2017.1286882)","Smart home devices are set to increase dramatically over the next decade. Alongside the proliferation of devices and the commercial opportunities and competition this expanding market generates, there are likely to be significant technological advances. 

Foremost among these is likely to be the increased coordination of devices across the home, such that they no longer operate as individual pieces of equipment, but as an integrated system of domestic provision. The interfaces associated with these devices will be integrated with the devices themselves (such as traditional buttons and dials, as well as smart screens and voice activation), but there is also likely to be the requirement for central 'hub' control of these devices (probably co-opting smartphones and tablets for this function). 

Given the domestic application of these devices, there may be a greater requirement for them to detect the mood and predict the behaviour of their human users. Hence, they may draw on a number of sensors to assist with this objective.","- These types of interfaces will have a significant impact on the way the internet of military things develops. One of the key concerns will be managing access to equipment nodes within a military network, ensuring they cannot be operated by the enemy.

- These interfaces will also be important for military accommodation, including onboard large naval platforms.","AI is playing a greater role in coordinating smart home devices and making the internet of things a more tangible concept. This is focused on analysing the data derived from a nexus of networked devices, drawing insights about their status and the activity of their users, and trying to deliver predictive function to the devices themselves. AI will also be involved in delivering efficient performance across a large network of devices in a given geographical region (e.g. a city).","The primary issue related to smart home devices is the extent to which capturing and analysing summative information taken from a domestic setting via these interfaces constitutes an invasion of privacy. There also issues about the storage, ownership and access rights to such information.","Smart devices are becoming increasingly prevalent, with many more consumer, professional and industrial grade products having smart and connected functionality incorporated into their design. This is likely to increase costs in system integration and assurance during procurement (organisation and infrastructure), as these devices will require more comprehensive cyber aware testing. It is also likely, however, that procuring solutions without these features, or adapting their designs to exclude them, will become increasingly expensive. Developing a proper assurance and vulnerability assessment process will therefore be critical.","Traditionally smart interface devices use audio and visual input and outputs. Some smart devices have no special interface at all, rather they adapt their normal function based on user behaviour. These systems use sophisticated processing and machine learning to capture and adapt to user behaviour.","Smart home devices are being produced by a large number of companies. Smart thermostats were one of the first applications, where [Hive](https://www.hivehome.com/) were an early market leader. [Samsung's smart fridge](https://www.samsung.com/uk/refrigerators/family-hub-fridge-freezers/) is a prominent example of a smart domestic appliance.",Step,Audiovisual,Land:Air:Maritime,../images/tech-images/base/18 - Smart home devices.png
19,Rapid 3D modelling,rapid_3d_modelling,19 - Rapid 3D modelling Black-01,Equipment which manufactures physical objects based on human input at a sufficiently rapid rate to allow a very short feedback loop between design/input and object creation.,3 Years,5 Years,10 Years,"Prototyping of engineering designs.

Experimental art and design.","- [Military applications of 3D printing](https://www.engadget.com/2017-08-14-3d-printing-revolutionize-marines-corps-fights.html)
- [Digital clay](https://www.youtube.com/watch?v=gkjhwhgAbT8)
- [Integrating feedback into 3D printing design cycle](https://www.accenture.com/_acnmedia/accenture/conversion-assets/dotcom/documents/global/pdf/dualpub_23/accenture-engaging-customer-influence-in-product-design-with-3d-printing.pdf)","As the speed of 3D manufacturing starts to increase and the cost of materials and production are driven down, it is reaching the stage where objects can be produced sufficiently rapidly that they can be considered as a method of dynamically relaying information to a human user. 

Furthermore, advances in the technology for sensing human implemented changes to the manufactured material itself, are able create a two way feedback loop. The manufactured material and the 3D printer can, therefore, be viewed as human-machine interfaces. 

Materials such as digital clay offer the potential for humans to rapidly generate, adapt and then signal changes to a machine system, which might be part of a design and production cycle. Alternatively, this feedback loop could be used as a way for a user to signal a particular instruction relating to a physical object (e.g. by marking the part of a simulacrum which needs attention in the real-world).","- This technology is likely to be relevant for testing designs and reporting damage for military platforms.

- This might be used in the context of operational planning, where physical features are modelled and adapted in response to information collected in the field.","Software is central to the delivery of 3D printing, in terms of computer aided design of the printed objects, but also in programming the manufacture. AI is likely to come into the workflow in two main places, first in supporting a human-centred design process, and second in terms of conducting quality assurance on the manufactured items.","The use of rapid 3D modelling and subsequent collaborative amendments of design objects, which are instantly reabsorbed back into the design process may pose a number of difficult challenges related to the ownership of intellectual property - especially where AI has assisted the process.","Rapid 3D modelling will have a profound impact on both procurement processes (organisation) and logistical support. It is likely to have a positive effect on the design and development of novel and bespoke equipment, as it facilitates rapid, iterative design. Furthermore, it will democratise and distribute manufacturing of small volume items and prototypes, making production of such items much more widely accessible. This will have significant impacts on supply chains and distribution networks. The supply chain will shift from having to procure, store and deliver a huge number of items, to merely supplying stocks of raw materials. However, significantly different skills will be required by operators of such equipment to leverage their full benefits, which will have implications for personnel recruitment and training.","3D modelling relies on the manipulation of a substrate material (or multiple materials) into a specific shape. The process of forming the substrate may be additive (where the material is applied to itself in layers) or subtractive (where it is machined from a block of material), or a combination of both. Additive processes may involve cooling of heated material, chemical curing of a material and a setting agent, or heat curing of a liquid substrate by a laser or other heating system. Subtractive processes are similar to traditional manufacturing processes such as milling, drilling, extrusion and erosion.","[Marektbot's Replicator 3D printer](https://www.makerbot.com/3d-printers/replicator/) is a high profile example of this technology.

The [Phrozen Sonic Mini](https://www.phrozen3dp.com/products/sonic-mini-lcd-rapid-3d-printer?sl-ref=all3dpphrozensonicmini&locale=en) is one of the first small 3D printers.",Incremental,Tactile,Land:Air:Maritime,../images/tech-images/base/19 - Rapid 3D modelling.png
20,Haptic feedback equipment,haptic_feedback_equipment,20 - Haptic feedback equipment Black,"Equipment that delivers haptic feedback to users (including vibration, thermal, pressure, friction and other neural stimuli).",3 Years,5 Years,8 Years,"Haptic suits for virtual reality gaming.

Assisting the visually impaired with experiencing visual models.","- [Thermal and pneumatic gloves](https://dl.acm.org/doi/fullHtml/10.1145/3355049.3360529)
- [Haptics to promote learning outcomes](https://www.mdpi.com/2414-4088/1/4/31/pdf)
- [Haptic feedback for remote UAV piloting](https://books.google.co.uk/books?id=BZ0cAwAAQBAJ&pg=PA2&lpg=PA2&dq=military+exploitations+of+haptics&source=bl&ots=6U8UB9UoZO&sig=ACfU3U0542y8r8d8Qf2u7pERqZFTDEm5Cw&hl=en&sa=X&ved=2ahUKEwjVm_m_juDoAhWkZxUIHbveBvoQ6AEwEnoECAsQLQ#v=onepage&q=military%20exploitations%20of%20haptics&f=false)
- [Haptics and machine learning](https://www.frontiersin.org/articles/10.3389/fnbot.2019.00051/full)","Haptic feedback equipment covers a wide range of underlying technologies, including vibrotactile feedback, pneumatics, thermal generation, airflow, electrical muscle stimulation and skin deformation. Collectively these technologies deliver a range of haptic effects to the user, including the sensation of touch, impact, temperature and resistance. 

These effects are currently used primarily to convey physical effects in virtual environments; however, they are increasingly being investigated as a way to increase the human sensory bandwidth, and convey other information to users about the status of the system, or the need for user attention or action. Most haptic effects are delivered via wearable technologies, such as haptic suits, but as the technology advances, it is possible that haptic feedback will begin to shift towards direct somatosensory stimulation. Safe, direct somatosensory stimulation could create novel effects such as returning the sensation of feeling to individual's with neural damage, or creating phantom sensations in the uninjured.","- Haptic feedback is already used as a feedback mechanism for pilots.

- It could be used to improve the effectiveness and realism of virtual training as a simulated alternative to high cost training activities such as parachuting.

- Virtual environments have been trialled in the treatment of post traumatic stress disorder, haptic feedback could be used to create greater sensory realism and evoke a greater treatment response.","One of the key areas of development involves looking at the feedback loop between a robotic prosthetic or remotely operated robot, and the haptic interface with the human user, to ensure the haptic feedback is intuitive and mimics human somatosensory experience of the real-world. This involves using advanced machine learning to infer how much pressure should be delivered via the haptic interface, based on the limited information collected by the often sparse (in comparison to the density of receptors in the human body) sensors on the robot or robotic component.","The added realism these interfaces could facilitate for virtual environments, leads to some interesting ethical questions about trauma experienced in a virtual environment. For example, a character dying in a simulation could become much more traumatic for the user if haptic feedback is involved. Additionally, haptic feedback could be used to produce user discomfort (too hot or cold, constriction or even pain). The extent to which discomfort is permissible via these interfaces will need addressing.","These interfaces present technical integration challenges, particularly with older legacy hardware (equipment). In service dismounted soldier systems and vehicles have significant load placed on their internal networks, which these interfaces will exacerbate. Requirements for novel interfaces should be considered in advance, during base platform acquisition, to enable future growth. The Impact on training is likely to be low, as these interfaces are designed to make use of innate human sense, so the additional information provided to operators is likely to be highly intuitive.","Basic haptic feedback systems are already commonplace in many commercial products such as smartphones and appliances, using electromechanical and solid state actuators to provide vibration and physical resistance feedback to the user. They also draw on pneumatics, thermal generation, air flow delivery, magnetic resistance and a wide range of other technologies. The input to these devices relies on significant amounts of timely processing to deliver meaningful feedback to the user.","[Valkyrie Industries](https://www.valkyrieindustries.co.uk/) are currently developing a haptic suit for use in an industrial setting, rather than for gaming. 

However, most applications continue to be in a gaming context, where [Teslasuit](https://teslasuit.io/) is a key supplier.",Incremental,Tactile,Land:Air:Maritime,../images/tech-images/base/20 - Haptic feedback equipment.png
21,Physical/tactile displays,physical_tactile_displays,21 - Physical n tactile displays Black-01,Physical displays or objects which can dynamically change shape based upon computer input and potentially be interacted with.,3 Years,7 Years,12 Years,"Providing tactile representations of shapes for the visually impaired.

Rapid modelling of physical objects to support storytelling or art.","- [Dynamic 3D shape display](https://tangible.media.mit.edu/project/inform/)
- [Small haptic arrays for use in a touchpad](https://dl.acm.org/doi/10.1145/3025453.3025971)
- [Haptic displays on smart home devices for visually impaired](https://www.researchgate.net/publication/329935988_Haptic_Display_Unit_IoT_For_Visually_Impaired)","These physical displays are often based on arrays of tactile pixels or 'taxels' (small nodules), which are either raised or depressed using electromagnetic actuation based on system input. These can be felt and interacted with by the human user. Principally designed to assist the visually impaired, they can also be used to present adaptable physical shapes that could be used in virtual environments to represent different objects. 

However, as dynamic shape displays increase in both sophistication and resolution, it is possible that these will come to have a broader set of functions. In addition to these rectangular 'flat' displays, other morphing technologies are starting to emerge that allow interfaces to change shape, for example morphing from a phone shape to a wristband shape depending on user needs. This could start to dramatically transform the versatility of devices.","- Tactile displays could be used to rapidly model physical environments for planning purposes, such as a small urban area.

- Potentially being able to dynamically alter the shape of pieces of equipment could increase multifunctionality, which would have implications for use cases where there are significant limitations on equipment weight (e.g. dismounted soldiers).",Software is required to control the physical actuators and also process any touch input from the human user. AI may be required to interpret this touch input and respond via the haptic display accordingly.,There are no assessed significant legal and ethical issues relating to the use of physical/tactile displays.,"Currently, these interfaces are novel, and there are limited commercial-off-the-shelf (COTS) and free and open-source software (FOSS) solutions to support their development, which will increase procurement costs (organisation). They are physically more bulky and complex than standard display counterparts, which will have a knock on effect for logistics as well as systems integration (e.g. size, weight and power (SWaP) requirements).","A wide range of technologies can be applied in these systems.The feedback may be electromechanical, with physical actuators providing changes to the shape and texture of the surface. Electrical stimulation of the skin can also provide the sensation of touch. In other approaches sonic devices can provide a physical sensation of texture by stimulation of the skin in a sonic field or constructively and destructively interfering sound waves.","The [inFORM 3D display](https://tangible.media.mit.edu/project/inform/) is the most advanced example of this technology, developed as a spin out from the Massachusetts Institute of Technology (MIT).",Incremental,Tactile,Land:Air:Maritime,../images/tech-images/base/21 - Physical n tactile displays.png
22,Visual/light indicators,visual_light_indicators,22 - Visual n Light Indicators Black-01,Equipment or objects which use integrated lights to relay information to a human user.,0 Years,2 Years,5 Years,"Warning lights and indicators on mechanical equipment.

Live entertainment shows.","- [The use of warning lights on vehicles considered within a legal context](https://www.researchgate.net/publication/281455838_THE_SCIENCE_OF_WARNING_LIGHTS)
- [Utilising predictive analytics for optimising maintenance of vehicles](https://www.researchgate.net/publication/281455838_THE_SCIENCE_OF_WARNING_LIGHTS)","This category of interface includes both traditional light-emitting diode (LED) displays (such as those found on instrumentation panels) and modern, aesthetic light display technologies incorporating flexible illuminated materials. These are primarily employed to signal to a human operator that something requires their attention (such as an indicator light), but might be used increasingly in more nuanced ways, for example, to influence the mood of the human operator. 

Advances in the resilience, flexibility, lifetime and miniaturisation of lights offer a number of previously impractical applications for using lights and light arrays for signalling information to humans (for example fitting disposable equipment with versatile light indicators which provide information about the status of the equipment - e.g. whether a hot drink in a container is cool enough to drink).","- Intelligent adaptive light displays could be used to help guide skilled but error-prone activities, such as military equipment repair.

- Smaller and more versatile lighting could mean that all person-borne equipment could be fitted with a series of informative indicator lights, which help users understand their status.",There may be some human behavioural analysis required to adjust the amount or frequency of visual indications to respond to user mood or context (e.g. filtering out all but the most important alarms in a high pressure situation).,"The main legal issues relating to visual/light indicators stem from the culpability of those who do not observe them, or fail to act in response to them. In this context, careful consideration must be given to what it is reasonable to expect a user to observe and monitor.","The use of visual indicators across Defence is already widespread, so there are few additional support requirements anticipated for the emerging developments in this area. The impact on personnel will need to be considered using established design principles and processes to ensure information overload is not introduced and ambiguity is minimised.","Indicator lights can be simple LED devices, but may also include laser, chemiluminescence and electrochemiluminescence devices. Future systems may incorporate electro-optical devices to provide light outputs which adjust to ambient light conditions to become more easily visible, or present interesting adjustable visual effects, such as iridescence (e.g. by potentially drawing on tunable diffraction gratings or photonic crystals).","Most indicator light developments are driven by the automobile industry, where [Audi](https://www.audi-mediacenter.com/en/audi-presents-new-technologies-at-the-ces-2628/lighting-technology-2661) are a notable innovator.",Incremental,Visual (non-text),Air:Cyberspace:Electromagnetic:Land:Littoral:Maritime:Space,../images/tech-images/base/22 - Visual n Light Indicators.png
23,2D visual displays,2d_visual_displays,23 - 2D Visual displays Black-01,Flat visual displays using a screen or another flat projection medium.,0 Years,0 Years,2 Years,"Data analytics displays.

Entertainment for film and TV.

Business presentations to audiences.","- [Military requirement for organic light-emitting doiode (OLED) screens](https://www.militaryaerospace.com/sensors/article/16721982/air-force-surveys-industry-for-companies-to-build-oled-micro-displays-for-military-electronics-upgrades)
- [Transparent display technologies](https://www.lumineq.com/blog/pros-and-cons-four-transparent-display-technologies-video-included)
- [Rugged design of screens](http://mil-embedded.com/articles/rugged-smart-military-displays-and-their-commercial-influence/)","Display screens have a very long history as a form of human-machine interface, and aside from improvements in the quality of image and reduction in the bulkiness of the screens, the primary advances in this interface technology have come from software developments. 

In terms of traditional screens themselves, the main advances are likely to be in increasing flexibility of screens (e.g. the introduction of roll-up or crushable screens) and also improved visibility through all light conditions (e.g. by drawing on organic light-emitting diode technology). 

Another key area of development is likely to be 2D projection technologies, for inclusion in interfaces. Features such as optical camouflage, algorithmic adjustments for surface irregularities and the miniaturisation of projection technology, will allow the human body and other irregular surfaces to be easily used as an interactive visual display. It is possible that 2D display technology will move beyond the requirement for a physical screen altogether, drawing on highly portable and adaptable projection technologies.","- 2D visual displays are already ubiquitous in Defence. However, their bulkiness and rigidity means they are not routinely carried by personnel. Flexible screen, might make the carrying of larger, dynamic digital map screens possible.

- Alternatively, projection technologies could remove the need for screens where they are used, freeing up space on board platforms and within offices, allowing almost limitlessly flexible working space arrangements.",A lot of software already exists to aid with visualisation and rendering of data. Significant software / hardware processing also underpins the generation of realistic lighting and texturing effects in rendered images (e.g. with graphics cards or software equivalents). New approaches for graphics rendering (e.g. ray tracing) are increasingly becoming mainstream.,2D visual displays have been in use for a long time and there are no anticipated novel legal or ethical issues related to their continued development.,"Display interfaces of this type are commonplace in many defence systems, and as such current procurement frameworks and supply chains are proficient at designing, delivering and supporting systems which incorporate them. However, increasing pixel density and resolutions increases demand for communication bandwidth and computational capacity and the information and infrastructure requirements will need to be factored in. Furthermore, novel types of display, using new materials are less well understood by designers and manufacturers, and may give rise to unforeseen logistical or infrastructure requirements.","2D displays may incorporate a wide range of underlying technologies. The light of each pixel may be generated by a number of different mechanisms such as a cathode ray tube projecting electrons onto a luminescent material, or electronically such as in LED and OLED pixels. Light crystal displays (LCDs) use filters to selectively remove frequencies of light produced by a backlight. Other technologies include electronic paper, which uses small cells containing material which, when electrically excited, expose or hide pigment. Organic chemicals may also be used in future technologies. Solid state or electro-optical devices have been imagined which mimic biological systems to dynamically alter the colour of pixels on a sheet. Many traditional display systems rely on a configuration of rigid materials. There is a race in commercial companies to develop flexible displays technologies which can be created in flexible polymers and other such materials.","Samsung and other mobile phone producers have been a leading developer of [OLED screens](https://www.oled-info.com/oled-devices/mobile-phones) in recent years, through a range of products.

Sony have been one of the main developers of 2D projected screens, with their [Xperia prototype](https://developer.sony.com/develop/xperia-touch/).",Incremental,Visual (text):Visual (non-text),Air:Cyberspace:Electromagnetic:Land:Littoral:Maritime:Space,../images/tech-images/base/23 - 2D Visual displays.png
24,3D visual displays,3d_visual_displays,24 - 3D visual displays Black-01,Visual displays which enable images to be seemingly viewed in three real-world spatial dimensions (as opposed to virtual 3D environments which are covered under virtual reality headsets).,3 Years,5Years,7 Years,"Entertainment in live shows or theme parks rides.

Collaborative design exploration in professions like architecture.","- [Holograms for military mission planning](https://www.dtnext.in/News/City/2018/04/15004838/1068954/3D-hologram-can-enhance-military-mission-plans.vpf)
- [Holograms for teaching](http://pen.ius.edu.ba/index.php/pen/article/view/441/325)
- [Commercial development of 3D projector](https://www.ft.com/content/1a9d3bec-ffbc-11e9-a530-16c6c29e70ca)
- [Tactile 3D displays using acoustic levitation](https://www.nature.com/articles/s41586-019-1739-5.epdf?referrer_access_token=TKrpjnNoZRfadLGY0vUYQNRgN0jAjWel9jnR3ZoTv0OWZkVAxFvzAstMlebCD6VfAD2R5YaIAnz84pNRISajZtOfAJp5dydE_LLW2vdsqHX655Aldfy0ab1K-cu4xUmDJpmqztpxZEZR29QJjVSub-Z3R50b_BpVqKwpwBmEMkOGhkqD1XcXtA8wZX5NeKpFMomy30sloGI4wwRPgbVHOj_WFCeMzjdFyC4G0FKAxEW52eTxY5DglW3L4qivjA8_1H9xzl7atsRdhdICzsjSqwGmT7BXeDAyavAGzzOVDIQ%3D&tracking_referrer=physicsworld.com)","3D visual displays are one of the fastest evolving areas of interface technology, which offer the opportunity to present visual information to users in a radically different way. 3D projectors are the most obvious candidate for bringing solid imagery into the real-world. These projections can be viewed from multiple directions and even interacted with. Not only does this offer the option for multiple users to easily observe the same display while standing around it (rather than all having to face in one direction), it also means that each user will get a different perspective of the image depending on their position. 

This may have profound implications for display interfaces, where different simultaneous user experiences may need to be managed across a group. Exploiting the increased reality of these images will also be key to their use, and novel applications are likely to arise. For example, the performances of deceased entertainment stars are now being brought 'back to life' by highly realistic holograms. Another area that might have utility in the context of interfaces is handheld perspective corrected displays, which present a 3D image within their own structure using synchronised imagery, thereby giving the illusion of holding a three dimensional object. There are also initiatives to develop tactile 3D displays using acoustic levitation.","- 3D display technologies are likely to be useful in terms of operational planning and considering terrain and other physical features.

- They will also have utility for equipment design and development, allowing potential users to examine the shape of equipment on different scales.

- This type of interface could also be used to create shared situational awareness, creating visualisations that could help to deliver the single information / intelligence environment.

- This technology would also allow mission footage to be played back in three dimensions.","Less software is readily available (compared to 2D graphics) for rendering images and representing data using 3D displays. This is likely to be a growth area as these interfaces increase in availability. Significantly more processing power required to represent 3D imagery, especially if it responds to user position. AI will play a role in systems which sense user position or movement and adjust the image accordingly.",The mounting realism and sophistication of 3D visual displays could give rise to some interesting ethical issues around the impact of viewing graphic or disturbing images. The potential for emotional or psychological harm may increase with their vividness.,"These types of interface present a number of integration challenges (organisation and equipment). For example, displays that must track the position and movement of the user will present challenges in certain operational environments (where space to set up equipment may be limited, or where positional tracking presents a security risk). Careful consideration of these restrictions will be required when designing new systems and integrating with existing systems. Additionally, these types of display will have significantly greater demand on computational capacity and communication bandwidth (infrastructure) than their simpler 2D counterparts.","Commercially available 3D displays usually rely on the observer to wear some form of glasses, which affect the image that each eye receives. Active glasses synchronise exposure of the left and right eye with two different images displayed alternatingly. Passive glasses rely on two images being displayed simultaneously, with polarisation shifted so they do not interfere. 3D displays which can be used without glasses can involve auto-stereoscopics, which project beams of left and right images to coincide with the axes of the observer's eyes. This can be achieved actively using eye detection and tracking systems, or passively using fixed interference optics. These approaches often rely on significant computational processing to generate the images and, in active systems, they respond to the user's movement and position. Other forms of display use lasers to illuminate particles of dust or ionise air molecules in space. Approaches, by their nature, may be single view or multi-view. Multi-view systems can provide a perception of parallax, causing a change in the image in response to observer position or movement.","[Voxon Photonics](https://voxon.co/]) are one example developer of commercial 3D projection technologies.

[MDH Hologram](https://mdhhologram.com/) were one of the first developers of realistic holographic projections for stage shows.",Incremental,Visual (non-text),Air:Cyberspace:Electromagnetic:Land:Littoral:Maritime:Space,../images/tech-images/base/24 - 3D visual displays.png
25,Virtual reality headsets,virtual_reality_headsets,25 - VR Black-01,Headsets that are worn in order to generate an immersive visual experience of a virtual world.,1 Year,3 Years,6 Years,"Virtual reality gaming.

Exploration of miniaturised or inhospitable environments for academic research.

Training for high risk activities (e.g. flying or surgery).","- [Virtual reality applications for the military](https://www.vrs.org.uk/virtual-reality-military/)
- [Analysis of different headset displays](https://arxiv.org/pdf/1912.02913.pdf)
- [Stereo blindness and 3D virtual environments](https://www.polygon.com/2016/11/7/13549156/virtual-reality-visual-impairment-vive-rift-playstation-vr)
- [Ethical concerns for virtual reality](https://technologyandsociety.org/virtual-reality-ethical-challenges-and-dangers/)","Virtual reality headsets have been in development for decades in the gaming industry. Their primary application is to create an immersive 3D visual experience, usually in combination with immersive sound. They are the primary interface for delivering virtual environments. 

There are likely to be incremental advances in reducing their bulkiness and increasing their portability and robustness. The increased integration of eye tracking technology will allow the displayed images to be adjusted based on point of gaze, and improve the quality and realism of the 3D aspect of the visual world. It is likely that many of the issues relating to motion sickness and disorientation that occur with moving around 3D virtual environments and also the problems of presenting 3D virtual environments to those with stereo blindness, are likely to be overcome in the next decades.

The most disruptive potential development would be the advent of direct retinal projection. This would effectively make the experience of viewing a virtual world indistinguishable from the experience of viewing the real-world - the only difference being the quality and fidelity of the graphics within the virtual world.","- Virtual reality headsets are already used to facilitate virtual activities such as wargaming, and enable personnel to train in 3D virtual environments.

- It is possible that these kinds of headsets are used to support the deployment of robotic proxies (for tasks such as mine disposal).

- Virtual reality headsets may become increasingly used for pre-deployment familiarisation with an alien environment.","AI is important for interpreting human movement and actions within the 3D virtual environment. Software to provide developers with tools to make use of virtual reality headsets is becoming increasingly prevalent - however, it is often proprietary to specific devices, and there are no universal open source common standards yet.","The primary concerns relating to virtual reality headsets stem from their role as the main interface used to access virtual environments. This means that most of the ethical issues associated with virtual reality will need to be dealt with in relation to this type of interface. These include ensuring the physical protection of someone operating in a virtual environment, considering the potential for 'virtual' crimes to be committed, the risk of trauma being experienced within the virtual world and the possibility that virtual images might be used to deliberately cause harm to a user, in effect amounting to 'virtual' torture.","Virtual reality headsets present and gather significant volumes of data (information). These data require a great deal of computational capacity, storage capacity and communication bandwidth. Furthermore, the communication latency requirements for headsets are highly critical. These issues will present significant challenges in systems integration, particularly with legacy platforms (equipment), as well as an increased demand on communication infrastructure.","Virtual reality systems employ a number of multi-sensory components to indicate user movement, position as well as provide simulate their senses. The user may receive information in the form of optical displays, status indicators, audio, vibration and pressure. A typical virtual reality headset and wand will detect position, orientation, movement, impulse (using solid state or electromechanical gyros, vibration sensors, compasses and other sensors) as well as squeeze pressure (e.g. piezoelectric or variable resistive/capacitive surfaces) and button pushes. It is likely that additional sensors will become commonplace. The input to these devices relies on significant amounts of timely processing to interpret the user's actions (or inaction) and serve this up in a form that software applications can make use of.","[Oculus](https://www.oculus.com/) are the market leader for the production of virtual reality headsets, but they have a growing number of competitors, such as [Valve](https://www.valvesoftware.com/en/index/headset) and [Vive](https://www.vive.com/uk/).",Incremental,Visual (text):Visual (non-text),Air:Cyberspace:Electromagnetic:Land:Littoral:Maritime:Space,../images/tech-images/base/25 - Vitrual Reality Headsets.png
26,Augmented reality glasses,augmented_reality_glasses,26 - Augmented reality Black-01,Transparent glasses (or visors) which are able to overlay visual or enhanced visual information over the real-world as seen by the human wearer.,2 Years,4 Years,7 Years,"In support of autonomous reality gaming.

Integrated with smartphones as a way of displaying information.","- [Military use of smart glasses](https://www.military.com/daily-news/2019/07/03/top-army-official-tests-out-futuristic-smart-combat-glasses.html)
- [Military applications of head mounted displays](https://www.janes.com/images/assets/174/50174/head-mounted_display_systems_for_dismounted_soldiers.pdf)
- [Thermal imager incorporated into head mounted display](https://www.defencetalk.com/bae-systems-to-deliver-head-mounted-displays-to-us-army-39844/)
- [Autonomous reality glasses in a medical setting](https://www.researchgate.net/publication/281788592_Use_cases_and_usability_challenges_for_head-mounted_displays_in_healthcare)
- [Ethical issues relating to smart glasses](https://oda.hioa.no/en/item/asset/dspace:18363/Smart-glasses%20Exposing%20and%20elucidating%20the%20ethical%20issues%20Hofmann%20et%20al%202017.pdf)","Augmented reality glasses (sometimes referred to as smart glasses), after some significant product launches in the mid-2010s, have become lower profile in recent years. A number of ethical, legal and technical issues limited their commercial viability and restricted adoption. However, development of the underlying technologies has continued. There is a history of using the same underlying technologies within the military, integrated into head or helmet mounted displays (particularly in military aviation). In the military context, this has begun to be integrated with enhanced vision effects, such as thermal imagery.

The key to solving most of the barriers to adoption will be solving the safety issues around divided attention, and also developing effective integrated interfaces that allow the user to interact with and manipulate the visual content presented via the glasses. An important enabler for meeting these challenges, will be improved eye tracking and focus estimation technologies. It is therefore likely that major development efforts for this technology area will focus on understanding exactly where a user is looking and what they want to see at that given moment. This could enable a seamless user experience, where vision of the real-world is not interrupted by presented information. In a similar way to virtual reality headsets, augmented reality glasses may increasingly draw on direct retinal display, making the sensation of viewing the presented information indistinguishable from viewing the real-world.","- This interface could be applied to assist with the targeting of a range of weapon systems.

- Provided information overlaid onto enhanced vision capabilities, such as thermal imaging.

- Smart glasses could be used to provide a dismounted patrolling soldier with relevant information about their surroundings.

- These interfaces could be used by a maintenance technician to assist them in repairing or servicing equipment.","AI is important for interpreting human movement and actions within the real-world, and overlaying visual images onto the real environment in a way which adapts to the real background. There will also be a requirement for the presentation of information to adjust to the real-world context, to allay safety concerns, such as not presenting information when the user is crossing a road.","The initial commercial launches of a range of smart glasses were controversial from the perspective of privacy concerns (particularly relating to the integration of cameras) and also safety (concerned with the split attention of users). These issues are likely to continue to arise as the technology develops. In the current data protection environment, there are also likely to be continuing concerns about the transparency of data collection by these devices. However, if direct retinal projection is used more widely, this may increase the capacity to diplay information privately to an individual, with only the user being able to receive the visual information.","These interfaces present and gather significant volumes of data (information), and are required to do so in a highly mobile way. These data require a great deal of computational capacity, storage capacity and communication bandwidth. Furthermore, the communication latency requirements for smart glasses are highly critical. These issues will present significant challenges in systems integration, particularly with legacy platforms (equipment), as well as an increased demand on communication infrastructure.","AR systems usually provide an overlay on top of an image of the observer's surrounding. The image of the surroundings may be directly detected by the observer (e.g. in the case of a HUD in an aircraft or Google Glass). Alternatively, it may be reproduced via a camera and visual display, as in the case of augmented reality apps on smartphones. Similarly to head mounted displays, augmented reality systems employ a number of multi-sensory components to indicate user movement and position. The user may also receive information in the form of optical displays, status indicators, audio and vibration. Eye tracking technology may also be used to try and better understand what the user is trying to see (the real-world or a projected display). The input to these devices relies on significant amounts of timely processing to interpret the user's actions (or inaction) and serve this up in a form that software applications can make use of.","There are numerous producers of smart glasses still in the market, including [North](https://www.bynorth.com/), [Vuzix](https://www.vuzix.com/products/blade-smart-glasses), [Solos](https://www.solos-wearables.com/) and [DreamWorld](https://www.dreamworldvision.com/).",Disruptive,Visual (text):Visual (non-text),Air:Cyberspace:Electromagnetic:Land:Littoral:Maritime:Space,../images/tech-images/base/26 - Augmented reality glasses.png
27,Audio indicators,audio_indicators,27 - Audio indicators Black-01,Equipment used to deliver sound-based information (other than speech) to users.,0 Years,1 Year,3 Years,"Sound warning indicators, such as car parking sensors.

Audio alarms for industrial workplaces.","- [A review of spatial audio techniques](https://www.mdpi.com/2076-3417/7/5/532/htm)
- [Acoustic warning indicators in the workplace where background noise is an issue](https://www.researchgate.net/publication/234155500_Detection_of_alarms_and_warning_signals_on_an_digital_in-ear_device)
- [3D sound for combat aircraft](https://www.militaryaerospace.com/communications/article/14069281/3d-audio-combat-aircraft-situational-awareness)","This covers interface technologies which deliver non-speech based sounds to a user in order to provide them with information in support of tasks. This category includes traditional audio indicators (such as sonar ping noises), but also the delivery of more complex sound information (such as audible morse code). Sound provides a number of variables which are intuitively interpreted by humans (e.g. pitch, cadence, volume), which makes it a useful medium for information transfer. 

Key developments in sound interfaces are likely to include: the introduction of body worn speakers (not earphones), where the sound is delivered directly to the surface of the body; advances in cochlear implants, potentially allowing virtual sounds to be delivered directly to the auditory nerve; the use of sound by machine systems to evoke emotional responses in human users (e.g. via music or ambient sound tones); and the development of audio signal processing, used to create a number of sound effects, most notably 3D sound, where the illusion of placing sounds in different areas can be generated in order to assist with human distinction between sound sources.","- Audio indicators could be used to provide a less binary indication of an approaching threat to, for example, a pilot. Different variables associated with sound, could be used to indicate a mounting or receding threat in an intuitive way.

- Sound could be used to help optimise the mood of personnel, relaxing them after strenuous activity, or energising them prior to vigorous activity.","There may be some human behavioural analysis required to adjust the amount or frequency of audio indications to respond to user mood or context (e.g. filtering out all but the most important audio indications in a high pressure situation). Additionally, synchronising audio indications will need to be performed to avoid clashes between multiple, simultaneous signals.","The main legal issues relating to audio indicators stem from the culpability of those who fail to act in response to them. In this context, careful consideration must be given to what it is reasonable to expect a user to hear and monitor. There may also be some ethical issues arising from attempts to manipulate the mood of users through sound.","This kind of interface could potentially pose a number of integration challenges. When integrating novel components into legacy systems (equipment) other auditory signals to the operator must be taken into account to prevent information overload and signal clashes (even those sound cues that are not delivered as an explicit sound indicator, e.g. the noise of an engine related to gear changes) . Furthermore, it is important that signals are unambiguous, particularly in safety critical applications. Currently, there are few common standards for audio signals, and very few that are ubiquitous. For example, there is no direct equivalent for a red visual signal indicating danger. Therefore new types of this interface will require training to avoid confusion. This could potentially be an area for development in doctrine, with the production of a universally across Defence systems.","Audio indicators will primarily use audio frequency range actuators to produce sound that the observer detects via their auditory senses. However, subsonic sound waves can give a physical sensation which is detected as pressure changes within the skin and deeper tissues. Directionality can be achieved by combining the audio signal of multiple actuators to constructively or destructively interfere. Additionally, sophisticated audio processing can mimic the difference in timing and tonality that a user would experience of a sound in a 3D space.","[Hooke](https://hookeaudio.com/) are an example of a commercial provider of 3D sound earphones.

[Mimi](https://www.mimi.io/) are a company that focuses on providing individually tailored sound processing technologies.",Incremental,Audio (non-speech),Air:Cyberspace:Electromagnetic:Land:Littoral:Maritime:Space,../images/tech-images/base/27 - Audio indicators.png
28,Brain-Computer Interface,brain-computer,28 - Brain-Computer Interface-01,"Technologies that detect and interpret brain activity in order to provide user inputs to computing devices, and potentially provide input to influence brain activity.",3 Years,10 Years,20 Years,"Enabling those with paralysis to operate equipment.

Supporting intuitive and near instantaneous option selection (thought-to-command)

Emotional sensing and mood management

Providing neurocogitive feedback to enhance cognitive performance","- [EEG brain-computer interfaces](https://journals.sagepub.com/doi/full/10.26599/BSA.2018.9050010)
- [Composing music with brain-computer interfaces](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0181584)
- [Military considerations for brain-computer interfaces](https://thejns.org/focus/view/journals/neurosurg-focus/28/5/2010.2.focus1027.xml)
- [Monitoring brain activity during firearms exercises](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7179757/)","This primarily relates to technologies utilising interfaces which use electroencephalogram (EEG). At present most of these are non-invasive and sit on the surface of the skull detecting brain activity. However, there are increasingly sophisticated partially and fully invasive interfaces that are implanted inside the skull or even within the brain tissue itself.

As the risk posed by the surgical implanting process reduces (presumably as the technology advances and confidence in the installation procedures increases), then these technologies will start to open up a number of very disruptive options, such as the direct reading of human intent. Research into P300 brain-computer interfaces, which offer the possibility of reading human attentional focus or even selection of options directly from brain activity, has increased and has the potential to allow people to write, paint and even compose music through thought alone. Furthermore, some experiments have looked at the impact of stimulating the brain directly on factors such as attentiveness. A better understanding of the neurology of cognition and more precise delivery interfaces, offer the potential that human thought could one day be directly stimulated, or more likely brain activity modulated in some way (e.g. in the treatment of psychiatric disorders). The miniaturisation of the components of such interfaces and the use of organic computing technologies may enable BCIs to be more easily and extensively integrated into the human brain.","- BCIs could be used to provide high-level directions for remotely operated platforms.

- Could be used in the treatment of physically injured service personnel (e.g. in support of robotic prosthetics or to pass neural signals to real limbs in the event of spinal cord damage).

- Could be used to treat mental illness by stimulating certain parts of the brain to alleviate symptoms.","Interpreting human brain activity and inferring intent is likely to be a highly complex analytical task, which will require advanced AI to carry out. This challenge could potentially reduce as the interfaces themselves get more sophisticated. A significant proportion of the analytical effort is currently allocated to interpreting noisy signals generated using surface EEGs which are imprecisely placed. Better understanding of the brain and the more direct placement of BCIs within the skull, would mean clearer signals. However, the richness and complexity of the data is likely to increase at this stage. Its full exploitation will require advanced analytic capabilities.","One of the primary issues likely to be of interest to the public will be the intimacy of the information being obtained by these interfaces, which comes directly from our brain. This is likely to raise concerns about the potential for reading and even controlling thoughts. It may also prompt more fundamental questions about mental agency, and whether intent is the same as action. A legal framework to reassure the public that these interfaces will be safe and respect the boundaries between thought and action, will be essential to enable their acceptance and use. Additionally, there are unprecedented privacy challenges to be considered relating to the potential to gather information directly from the human brain. Even though the technology is unlikely to be able to 'read minds' in the near future, establishing differences between neural and mental data will be crucial as the technology develops.","These interfaces, if used in a military context, are likely to have profound effects on doctrine development (concepts). The ability to issue commands to equipment without an explicit, physical step (such as pressing a button) will require a new set of operating procedures to be developed in many contexts. Training will be central to the deployment of these interfaces, as the use of thought for the direction of equipment is likely to be unfamiliar to most personnel. Building confidence in the reliability of the interface (particularly in stressful situations) will be essential if it is ever to be deployed. ","BCIs are based on electroencephalography (EEG), which detects the electrical activity of neurons within the brain. Through the interpretation of the sensed activity it is possible to determine the desired output of the brain (e.g. movement, speech, even thought). However, our understanding of human brain is limited by its massive complexity and our restricted access. One of the key developments in this area of interface development is the attempt to produce more sophisticated sensors that can connect with the neurons more directly. These is seeing a move away from non-inavsive interfaces (such as surface EEG hoods) towards implanted interfaces (placed within the skull) and even materials that can have much greater connectivity with brain tissue (such as neural laces which are currently under development).","[Neuralink are one of the most high profile companies investing in BCI technology development](https://neuralink.com/)
[Muse have developed a brain sensing headband to assist with meditation](https://choosemuse.com/)",Disruptive,Electrical Signal,Air:Cyberspace:Electromagnetic:Land:Littoral:Maritime:Space,../images/tech-images/base/28 - Brain Computer Interface.png
29,Olfactory Interfaces,olfactory,29 - Olfactory Interfaces-01,Interfaces that deliver smell to users in order to influence their mood or relay information.,2 Years,7 Years,10 Years,"Supporting interactive museum exhibitions or art installations

In association with home entertainment systems

Mood-driven air fresheners","- [Scent releasing wearables influenced by biophysical sensors](https://www.media.mit.edu/publications/essence-ol/)
- [Utilising smell to influence dream state](https://www.media.mit.edu/projects/closed-loop-olfactory-interfaces/overview/)
- [On face wearable scent delivery mechanisms](https://dl.acm.org/doi/abs/10.1145/3313831.3376737)","This includes interface technologies that deliver scent to users in order to influence their mood (perhaps in combination with biosensors which detect psychophysiological information about the user's mood state) or relay important information (e.g. regarding environmental safety factors). These interfaces may take the form of wearable devices, or could be integrated into non-portable domestic infrastructure. 

At present, smell is an underutilised modality for the delivery of information. It is also one which has the potential to unlock more emotional or memory-based responses in users than other mechanisms of delivery. Future developments in this area could involve the enhancement of a user's sense of smell, by magnifying scent based on chemical detection by synthetic sensors, or the delivery of smell to augment virtual environments. Integration with information derived from the user via biosensors may have the potential to allow smell to be used to help influence the mood of the user (e.g. by helping to reduce stress or by increasing attentiveness). Some work is examining the impact of delivered scents on dream state. In combination with the monitoring of brain activity, this could lead to a feedback loop which allows dreams to be influenced.","- Using the delivery of smell to relay CBRN hazards to deployed personnel (e.g. a distinctive smell was released when a odourless chemical agent was detected by worn sensors).

- The use of scent to relax deployed personnel and help them sleep better.

- Issuing scent within military vehicles to increase the alertness of drivers.","The key supporting requirements for this type of interface are likely to relate to the production of scent itself and also interpreting the mood state of the user. In order to deliver bespoke scent to individual users depending on their requirements, any system is going to need to be able to take data from sensors and interpret this in terms of the user's mood state. Many other types of interface have a potential requirement for interpreting mood state, so it is possible that olfactory interfaces would simply draw on a single data analysis and AI capability for this function and simply receive mood state as an input from this central function.","Delivering smell to a user is unlikely to be perceived as high risk or intrusive in any way, even if one of the aims of doing so is to manipulate mood. In essence, this function is no different to the use of air fresheners. Others than assurances that the scent delivered will not be harmful, there are no foreseen major legal or ethical implications for this interface technologu=y.","The logistics of keeping the scent reservoirs required for the delivery of smell full is likely to prove extremely challenging. Each interface is likely to require small amounts of chemicals, which would be mixed by the scent delivery device itself. For the device to be practical, it will necessariy be small and only ever be stocked with small amounts. Resupply would therefore be a constant issue. It could be that these restrictions would prevent this tye of interface from ever being used by disparate, deployed personnel. Instead, it is more likely to be restricted to central sites, with static facilities, or within military vehicles, where the existing supply infrastructure could easily be used.","Olfactory interfaces can work in a variety of ways, and are primarily based on conceptual designs at the moment. They generally include some sort of flow delivery system (which channels the scent to the nose via pumping or some other mechanism), a pallete of odor generating substances (which stores and mixes the scent before delivery) and algorithms which calculate the right mix of subtances to generate the desired scent (including its magnitude).",[The Scentee Machina is a multiple scent delivery device that allows a range of smells to be delivered into a space depending on different requirements](https://scentee-machina.com/),Incremental,Chemical Senses,Air:Cyberspace:Electromagnetic:Land:Littoral:Maritime:Space,../images/tech-images/base/29 - Olfactory Interfaces.png
